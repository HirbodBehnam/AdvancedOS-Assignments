\section{stress}
در ابتدا بر روی کامپیوتر خودم دستورات ذکر شده را به کمک اسکریپت
\lr{3.sh}
اجرا کردم. نتیجه‌ی آنها در جدول
\ref{table:bare_metal_stress}
آمده است.
\begin{figure}[H]
    \begin{latin}
        \centering
        \begin{tabular}{|c|ccc|}
        \hline
        Type & TLB Shootdown & TLB Miss & Pagefault\\
        \hline
        CPU & 3105 & 476136 & 322\\
        \hline
        Memory & 7266&148106973&67786599\\
        \hline
        IO & 14131 & 73119467 & 223\\
        \hline
        \end{tabular}
    \end{latin}
    \caption{جدول مربوط به اجرای برنامه‌ی \lr{stress} بر روی کامپیوتر خودم با پارامتر‌های مختلف}
    \label{table:bare_metal_stress}
\end{figure}
حال دقیقا همین اسکریپت را همزمان بر روی دو
\lr{HPC}
مربوط به درس اجرا می‌کنیم.
\begin{figure}[H]
    \begin{latin}
        \centering
        \begin{tabular}{|c|cccc|}
        \hline
        Type & \multicolumn{2}{c}{TLB Shootdown} & \multicolumn{2}{c|}{Pagefault}\\
        \hline
        Machine & VM1 & VM2 & VM1 & VM2\\
        \hline
        CPU & 13794 & 13753 & 297 & 295\\
        \hline
        Memory & 5995 & 4049 & 14869079 & 15718735\\
        \hline
        IO & 6464 & 6541 & 229 & 225\\
        \hline
        \end{tabular}
    \end{latin}
    \caption{جدول مربوط به اجرای برنامه‌ی \lr{stress} بر روی ماشین‌های مجازی به صورت همزمان}
    \label{table:vm_stress}
\end{figure}
در نهایت دوباره آزمایش را انجام می‌دهیم ولی این بار فقط بر روی یک ماشین مجازی اجرا می‌کنیم.
\begin{figure}[H]
    \begin{latin}
        \centering
        \begin{tabular}{|c|cc|}
        \hline
        Type & TLB Shootdown & Pagefault\\
        \hline
        CPU & 14074 & 298\\
        \hline
        Memory & 5995 & 25142613\\
        \hline
        IO & 5933 & 233\\
        \hline
        \end{tabular}
    \end{latin}
    \caption{جدول مربوط به اجرای برنامه‌ی \lr{stress} بر روی ماشین‌های مجازی به صورت تکی}
    \label{table:vm_stress_single}
\end{figure}
به نظر من این نتایج مقداری عجیب است. مثلا در ماشین‌های مجازی تعداد
\lr{page fault}ها
نسبت به حالت
\lr{bare metal}
بسیار کمتر است. و چیزی که عجیب است این است که تعداد
\lr{TLB miss} و \lr{shootdown}
در زمان
\lr{CPU load}
نیز اصلا کم نیست! من از روی کنجاکاوی به سورس کد برنامه‌ی
\lr{stress}
نگاهی انداختم. سورس کد برنامه در
\link{https://github.com/resurrecting-open-source-projects/stress/blob/master/src/stress.c}{این لینک}
موجود است.

در ابتدا با توجه به تابع
\lr{main}
می‌توان دید که مثلا برای حالتی که
\lr{CPU bound}
هستیم صرفا به کمک
\lr{fork}
چندین پردازه‌ی جدید را ایجاد می‌کنیم و در همه‌ی آنها تابع
\codeword{hogcpu()}
را صدا می‌کنیم. این تابع صرفا در یک حلقه‌ی بینهایت تابع
\codeword{sqrt}
را بر روی اعداد رندوم صدا می‌کند.
پس احتمالا تعداد زیاد
\lr{TLB shootdown} و \lr{TLB miss}
برای
\lr{context switch}ها
است.

قسمت
\lr{IO}
نیر به صورت خیلی زیادی مشابه قسمت
\lr{CPU}
است. تنها فرق آن این این است که به جای
\codeword{sqrt}
تابع
\codeword{sync}
را صدا می‌زنیم. حال در
\lr{man page}های
لینوکس سرچ می‌کنیم که این دستور چه کار می‌کند.
\begin{latin}
    \begin{quote}
        sync, syncfs - commit filesystem caches to disk

        According to the standard specification  (e.g.,  POSIX.1-2001),  sync()
       schedules the writes, but may return before the actual writing is done.
       However Linux waits for I/O completions, and thus  sync()  or  syncfs()
       provide the same guarantees as fsync() called on every file in the system or filesystem respectively.
    \end{quote}
\end{latin}
پس به صورت خلاصه این دستور کش‌های سیستم‌عامل را در دیسک می‌نویسید ولی احتمال دارد که قبل از نوشته شدن تابع برگردد.
مشاهده می‌شود که در حالت
\lr{IO intensive}
تعداد
\lr{TLB Miss}ها
افزایش می‌یابد. این موضوع به نظر من مقداری عجیب است چرا که اصلا ما کجا به مموری نیاز داریم که داریم
\lr{TLB miss}
می‌خوریم؟ من نهایت فکری که می‌توانم بکنم این است که برای این قسمت و قسمت قبل ممکن است که
\lr{context switch}ها
دلیل
\lr{TLB shootdown}
باشد.

برای مموری نیز مشخص است. به دلیل 
\lr{allocate}
و
\lr{deallocate}
کردن زیاد مموری ما
\lr{page fault}
می‌خوریم. اما چیزی که خیلی برای من جالب است این است که زمانی که بر روی یک ماشین مجازی به تنهایی 
\lr{stress}
را اجرا می‌کنیم تعداد
\lr{page fault}ها
به شدت زیاد می‌شود. این موضوع در ابتدا به نظر شاید عجیب به نظر برسد اما به نظر من دلیل این موضوع این است
که زمانی که فقط بر روی یک ماشین مجازی اجرا می‌کنیم
\lr{stress} را،
تعداد
\lr{iteration}
بیشتری می‌تواند صورت گیرد به خاطر کمتر
\lr{utilize}
بودن سخت افزار. پس بیشتر می‌‌توانیم
\lr{allocate} و \lr{deallocate}
کنیم و در نتیجه تعداد
\lr{page fault}ها
نیز افزایش می‌یابد! همچنین دقت کنید که جمع
\lr{page fault}های
دو ماشین مجازی تقریبا برابر با تک ماشین مجازی است.

\smalltitle{آپدیت بعد از درست شدن مقادیر \lr{perf}:}
جدول مربوط به اجرای
\lr{stress}
بعد از حل شدن مشکل
\lr{perf}
در شکل زیر آمده است.
\begin{figure}[H]
    \begin{latin}
        \centering
        \begin{tabular}{|c|cccccc|}
        \hline
        Type & \multicolumn{2}{c}{TLB Shootdown} & \multicolumn{2}{c}{TLB Miss} & \multicolumn{2}{c|}{Pagefault}\\
        \hline
        Machine & VM1 & VM2 & VM1 & VM2 & VM1 & VM2\\
        \hline
        CPU & 13910 & 14001 & 4871964 & 5465089 & 201 & 295\\
        \hline
        Memory & 5853 & 5876 & 189683947 & 209580532 & 23895020 & 24088448\\
        \hline
        IO & 6449 & 6221 & 7184217 & 6314307 & 127 & 125\\
        \hline
        \end{tabular}
    \end{latin}
    \caption{جدول مربوط به اجرای برنامه‌ی \lr{stress} بر روی ماشین‌های مجازی به صورت همزمان}
\end{figure}
چیزی که برای من خیلی جالب است این است که برنامه‌ی
\lr{CPU bound}
تعداد
\lr{TLB shootdown}
بیشتری نسبت به برنامه‌ی
\lr{IO bound}
دارد ولی تعداد
\lr{TLB miss}های
برنامه‌ی
\lr{IO bound}
بیشتر است! توجیه کردن قسمت
\lr{TLB miss}
برای من راحت‌تر است چرا که در برنامه‌ای که مربوط به
\lr{CPU}
است اصلا با مموری سر و کار نداریم! نهایتا همان طور که گفتم این عدد برای تعداد زیاد
\lr{context switch}ها
باشد. ولی برای
\lr{TLB shootdown}
زیاد ایده‌ای جز
\lr{context switch}
ندارم. تعداد
\lr{TLB miss}های
برنامه‌ی
\lr{memory bound}
نیز منطقی است. چرا که صرفا حجم مموری برنامه زیاد است و برای نوشتن و خواندن در آن مموری زیاد
\lr{miss}های
زیادی می‌خوریم. همچنین یک نکته‌ی جالب دیگر این است که تعداد این
\lr{TLB miss}ها
تقریبا برابر یک ماشین عادی است.